The Introduction should explain the principal motivation for the project. Show how the work fits into the broad area of surrounding Computer Science and give a brief survey of previous related work. It should generally be unnecessary to quote at length from technical papers or textbooks. If a simple bibliographic reference is insufficient, consign any lengthy quotation to an appendix. 

#############################################################################################

Computers are fundamentally bad at many real world tasks. While some previously difficult problems such as noisy communications channels have been alleviated by mathematical descriptions, problems with no clear solution at all are some of the most common and useful. For example, automating search queries requires a system designer to constrain user queries to unnatural, keyword based snippets, to intepret what a user is actually asking rather than what they have typed, to decide what makes an appropriate answer and how to strike a balance between time taken for indexing and coverage. The field of Artificial Intelligence attempts to stochastically solve some of these problems, while equally important in actual implementations is economics, politics, intellectual property and competition. 

A research topic which is just beginning to be incorporated into consumer goods is gesture recognition. Currently, easily available and widespread implementations exist in applications such as mouse gestures in the Opera web browser and accelerometer-based gestures in the iPhone, which aim to allow intuitive, natural interaction. 

Conversely, robots have generally been operated by skilled engineers and researchers. The science fiction of ubiquitous robots in the home relies on natural interaction with humans, without extensive training, and with the ability to adapt and learn online from imperfect and noisy information. Machine learning is a good fit for these parameters, and thus the ambition was to implement real-time gesture recognition for control of robots.

Ideally, such a domestic robot would recognise the presence of a household member, identify them  given a defined security policy or by making an intelligent decision for unknowns, intepret their speech, tone, facial expression and body language into a goal, then break down this into solvable subgoals. This dissertation simplifies the above goals by focusing on converting body language (specifically, arm gestures) into a command by pre-training a system with a limited set of pre-defined commands.

One of the constraints on such goals is the current state of machine vision. Apart from being a mathematically impossible goal (converting a 2D image into a 3D object), the latest algorithms still do not provide accuracy rates comparable to motion capture systems. For this reason, until the field advances a further 10 or 20 years, we restrict ourselves to using the relatively clean, smoothed data provided by the Vicon motion capture system rather than using a multi-camera setup.

For the purposes of exhibiting the capabilities and future opportunities, an application for collision-based interaction was created which requires multiple robots. An semi-immersive environment was created where each user used a projected live stream of video from their robot's point of view.

executive process
language for control of robots

#############################################################################################

The motivation of this project was to investigate different methods of gesture recognition as applied to real-time robotics control.

Artificial intelligence is the study of agents who transform information about their surroundings into actions to achieve a goal. Early AI, based on playing games such as chess, used heuristics together with search techniques under various optimizations (minimax and alpha-beta pruning) to exhaustively search the game tree.

This lead naturally on to machine learning, where instead of trying all possible paths, the computer could determine patterns in data based on seeing previous examples. Depending on whether these examples are labelled, supervised and unsupervised learning techniques have been used in data mining, natural language processing, search engines, financial fraud and many other applications. The key innovation was to use statistical methods, which allows use of imperfect data such as in many real world contexts.

Pattern recognition is a specific example of machine learning which  aims to classify data into categories or patterns. Image processing and speech and handwriting recognition are both commercially available examples of pattern recognition, the former in CCTV systems to recognize human faces and the latter for hands-free data entry. Typically, the raw data is processed using feature extraction, to reduce the dimensionality and thus allow faster processing.

Gesture recognition is temporal pattern recognition on hand, face or body gestures, typically to issue a command in real time rather than to classify a collection of data. Currently, easily available and widespread implementations exist in applications such as mouse gestures in the Opera web browser and the iPhone, which aim to allow intuitive interaction with a system.

Conversely, robots have generally been operated by skilled engineers and researchers. The science fiction fantasy of ubiquitous robots in the home relies on natural interaction with humans, without extensive training, and with the ability to adapt and learn online from imperfect and noisy information. Machine learning is a good fit for these parameters, and thus the ambition was to implement real-time gesture recognition for control of robots.

Specifically I designed a system using the Vicon motion capture system for data capture and its associated computer for the classifier. The robots used are iRobot Creates, connected to OLPC XO laptops for wireless control and webcam capabilities.
